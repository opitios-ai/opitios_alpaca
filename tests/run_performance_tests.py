"""
æ€§èƒ½æµ‹è¯•è¿è¡Œè„šæœ¬
æ–¹ä¾¿æ‰§è¡Œå„ç§æ€§èƒ½æµ‹è¯•å¥—ä»¶å’Œç”ŸæˆæŠ¥å‘Š
"""

import os
import sys
import subprocess
import time
import json
import argparse
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class PerformanceTestRunner:
    """æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, output_dir="performance_reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
    def run_test_suite(self, test_file, test_class=None, markers=None):
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        print(f"\n{'='*60}")
        print(f"Running performance tests: {test_file}")
        if test_class:
            print(f"Test class: {test_class}")
        if markers:
            print(f"Markers: {markers}")
        print(f"{'='*60}")
        
        # æ„å»ºpytestå‘½ä»¤
        cmd = [
            "python", "-m", "pytest",
            f"tests/{test_file}",
            "-v",
            "--tb=short",
            "--capture=no",  # æ˜¾ç¤ºprintè¾“å‡º
        ]
        
        # æ·»åŠ æ ‡è®°è¿‡æ»¤
        if markers:
            cmd.extend(["-m", markers])
        
        # æ·»åŠ ç‰¹å®šæµ‹è¯•ç±»
        if test_class:
            cmd.append(f"::{test_class}")
        
        # æ·»åŠ æ€§èƒ½æŠ¥å‘Š
        report_file = self.output_dir / f"{test_file.replace('.py', '')}_{self.timestamp}.json"
        cmd.extend(["--json-report", f"--json-report-file={report_file}"])
        
        try:
            # è¿è¡Œæµ‹è¯•
            start_time = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)
            end_time = time.time()
            
            # è¾“å‡ºç»“æœ
            print("\nSTDOUT:")
            print(result.stdout)
            
            if result.stderr:
                print("\nSTDERR:")
                print(result.stderr)
            
            # ä¿å­˜ç»“æœ
            execution_time = end_time - start_time
            self.save_test_result(test_file, result, execution_time)
            
            return result.returncode == 0
            
        except Exception as e:
            print(f"Error running tests: {e}")
            return False
    
    def save_test_result(self, test_file, result, execution_time):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        result_data = {
            "test_file": test_file,
            "timestamp": self.timestamp,
            "execution_time": execution_time,
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr
        }
        
        result_file = self.output_dir / f"{test_file.replace('.py', '')}_result_{self.timestamp}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nTest result saved to: {result_file}")
    
    def run_all_performance_tests(self):
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        test_suites = [
            {
                "file": "test_performance_load.py",
                "description": "Performance and Load Tests",
                "markers": "performance"
            },
            {
                "file": "test_account_pool.py", 
                "description": "Account Pool Performance Tests",
                "class": "TestConcurrencyAndRaceConditions",
                "markers": "performance"
            },
            {
                "file": "test_routing_load_balancing.py",
                "description": "Routing Performance Tests", 
                "class": "TestRoutingPerformance",
                "markers": "performance"
            },
            {
                "file": "test_websocket_connections.py",
                "description": "WebSocket Performance Tests",
                "class": "TestWebSocketPerformance", 
                "markers": "performance"
            }
        ]
        
        results = {}
        
        print(f"Starting comprehensive performance test run at {datetime.now()}")
        print(f"Results will be saved to: {self.output_dir}")
        
        for suite in test_suites:
            print(f"\n{'-'*60}")
            print(f"Running: {suite['description']}")
            print(f"{'-'*60}")
            
            success = self.run_test_suite(
                suite["file"],
                suite.get("class"),
                suite.get("markers")
            )
            
            results[suite["file"]] = {
                "success": success,
                "description": suite["description"]
            }
            
            if success:
                print(f"âœ“ {suite['description']} completed successfully")
            else:
                print(f"âœ— {suite['description']} failed")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_summary_report(results)
        
        return results
    
    def generate_summary_report(self, results):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        summary = {
            "timestamp": self.timestamp,
            "total_suites": len(results),
            "successful_suites": sum(1 for r in results.values() if r["success"]),
            "failed_suites": sum(1 for r in results.values() if not r["success"]),
            "results": results
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        summary_file = self.output_dir / f"performance_summary_{self.timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        self.generate_html_report(summary)
        
        # æ‰“å°æ€»ç»“
        print(f"\n{'='*60}")
        print("PERFORMANCE TEST SUMMARY")
        print(f"{'='*60}")
        print(f"Timestamp: {self.timestamp}")
        print(f"Total test suites: {summary['total_suites']}")
        print(f"Successful: {summary['successful_suites']}")
        print(f"Failed: {summary['failed_suites']}")
        print(f"Success rate: {summary['successful_suites']/summary['total_suites']*100:.1f}%")
        print(f"\nReports saved to: {self.output_dir}")
        print(f"Summary report: {summary_file}")
        
    def generate_html_report(self, summary):
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Performance Test Report - {summary['timestamp']}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ background-color: #e7f3ff; padding: 15px; margin: 20px 0; border-radius: 5px; }}
        .test-result {{ margin: 10px 0; padding: 10px; border-radius: 5px; }}
        .success {{ background-color: #d4edda; border-left: 5px solid #28a745; }}
        .failure {{ background-color: #f8d7da; border-left: 5px solid #dc3545; }}
        .metrics {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; margin: 20px 0; }}
        .metric {{ background-color: #f8f9fa; padding: 15px; border-radius: 5px; text-align: center; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
        .metric-label {{ font-size: 14px; color: #6c757d; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Performance Test Report</h1>
        <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>Test Run ID: {summary['timestamp']}</p>
    </div>
    
    <div class="summary">
        <h2>Summary</h2>
        <div class="metrics">
            <div class="metric">
                <div class="metric-value">{summary['total_suites']}</div>
                <div class="metric-label">Total Test Suites</div>
            </div>
            <div class="metric">
                <div class="metric-value" style="color: #28a745;">{summary['successful_suites']}</div>
                <div class="metric-label">Successful</div>
            </div>
            <div class="metric">
                <div class="metric-value" style="color: #dc3545;">{summary['failed_suites']}</div>
                <div class="metric-label">Failed</div>
            </div>
            <div class="metric">
                <div class="metric-value">{summary['successful_suites']/summary['total_suites']*100:.1f}%</div>
                <div class="metric-label">Success Rate</div>
            </div>
        </div>
    </div>
    
    <div class="test-results">
        <h2>Test Results</h2>
"""
        
        for test_file, result in summary['results'].items():
            status_class = "success" if result['success'] else "failure"
            status_text = "âœ“ PASSED" if result['success'] else "âœ— FAILED"
            
            html_content += f"""
        <div class="test-result {status_class}">
            <h3>{result['description']}</h3>
            <p><strong>File:</strong> {test_file}</p>
            <p><strong>Status:</strong> {status_text}</p>
        </div>
"""
        
        html_content += """
    </div>
    
    <div class="footer">
        <h2>Performance Testing Guidelines</h2>
        <ul>
            <li><strong>Connection Pool:</strong> Tests connection acquisition, release, and routing performance</li>
            <li><strong>API Endpoints:</strong> Tests request/response times and throughput</li>
            <li><strong>WebSocket:</strong> Tests real-time data broadcasting performance</li>
            <li><strong>Load Testing:</strong> Tests system behavior under high concurrent load</li>
        </ul>
        
        <h3>Performance Targets</h3>
        <ul>
            <li>API Response Time: &lt; 100ms (P95)</li>
            <li>Connection Pool Throughput: &gt; 1000 ops/sec</li>
            <li>WebSocket Broadcast: &gt; 1000 clients/sec</li>
            <li>Concurrent Users: Support 1000+ simultaneous users</li>
        </ul>
    </div>
</body>
</html>
"""
        
        html_file = self.output_dir / f"performance_report_{self.timestamp}.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"HTML report: {html_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Run performance tests for opitios_alpaca")
    parser.add_argument("--test-file", help="Specific test file to run")
    parser.add_argument("--test-class", help="Specific test class to run")
    parser.add_argument("--markers", default="performance", help="Test markers to filter")
    parser.add_argument("--output-dir", default="performance_reports", help="Output directory for reports")
    parser.add_argument("--all", action="store_true", help="Run all performance tests")
    
    args = parser.parse_args()
    
    runner = PerformanceTestRunner(args.output_dir)
    
    if args.all or not args.test_file:
        # è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
        results = runner.run_all_performance_tests()
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        success_count = sum(1 for r in results.values() if r["success"])
        if success_count == len(results):
            print("\nğŸ‰ All performance tests passed!")
            sys.exit(0)
        else:
            print(f"\nâš ï¸  {len(results) - success_count} performance test(s) failed!")
            sys.exit(1)
    else:
        # è¿è¡Œç‰¹å®šæµ‹è¯•
        success = runner.run_test_suite(args.test_file, args.test_class, args.markers)
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()